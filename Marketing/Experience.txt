I have around 8+ years of experience as Big Data developer. I started my career as a java developer and moved onto bigdata and Hadoop related ecosystem components and currently, primarily working on spark, using spark on a day to day basis for building high end data processing applications and with spark Iâ€™ve been using scala and python primarily, and few projects I use scala as well for building spark applications and apart from spark I have descent exposure to hive, mapreduce, impala, kafka, sqoop, hbase, yarn so these are some of the other tools in the Hadoop stack that I have descent exposure to apart from spark, spark still being the primary framework that I have been using on day to day basis, but apart from spark I have good exposure to other Hadoop ecosystem components for building the pipelines and managing and automating the pipelines.